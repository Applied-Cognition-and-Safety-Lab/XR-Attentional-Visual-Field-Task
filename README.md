<i> Visual attention is critical for everyday task performance and safety. It allows people to detect and process critical visual objects, information, and potential hazards in an environment to respond to. An individual’s ability to distribute visual attention in space can be assessed by a computerized task that measures an ability to detect and respond to visual targets in a wide area of one’s focal and peripheral visual field. The Attentional Visual Field (AVF) task is a computerized task that displays a visual stimulus on a computer screen, and people need to identify a target among distractors while attending to a large visual field. </i>

## Attentional Visual Field (AVF) Task

![1-s2 0-S0003687022001272-gr1_lrg](https://user-images.githubusercontent.com/105318271/193877517-d1339ffb-e356-497a-8121-3ec1c17871a9.jpg) 

Traditionally, an Attentional Visual Field task is developed for and conducted on a desktop computer. While completing this task, participants are instructed to keep their heads in a chinrest to ensure that the visual stimuli are displayed at the appropriate size and distance. Participants must also fixate on a central point on the screen around which the other visual stimuli are calibrated to be at certain distances from the center of the visual field. This task measures the spread of attention across the visual field by displaying targets at different locations in the visual field based on the target's direction and distance from the center of the visual field. Participants are instructed to indicate which direction the target appears in for each trial, and their accuracy and response time are used to describe the shape of their attentional visual field.
<br>
<br>

## XR Attentional Visual Field Task

<img width="410" alt="Screenshot 2022-10-06 144643 1632" src="https://user-images.githubusercontent.com/105318271/194405056-48a09161-5136-4298-980b-5557f87527e6.png">
A condensed version of the AVF task in the Virtual Environment.
<br>
<br>
This XR Attentional Visual Field Task is based on the computer-based AVF task. It has been developed to work with most VR and AR headsets by implementing the OpenXR API within the Unity3D game engine. This project uses Unity version 2020.3.7f1. Participants respond to the trials by using a numpad to indicate the direction that the target stimulus appeared in. For example, a response of 7 on the numpad would indicate that the target appeared in the top left or Northeast area of the visual field.
<br>

### Table of contents

The project is intended to work on the HTC Vive (Pro). Please refer to the according sections for the Setup Intrstructions.

- [Getting Started](https://github.com/Applied-Cognition-and-Safety-Lab/XR-Attentional-Visual-Field-Task/blob/main/Getting%20Started.md)
- [Experiment Settings](https://github.com/Applied-Cognition-and-Safety-Lab/XR-Attentional-Visual-Field-Task/blob/main/ExperimentSettings.md)
- [Data Output](https://github.com/Applied-Cognition-and-Safety-Lab/XR-Attentional-Visual-Field-Task/blob/main/DataOutput.md)


# Copyright

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), 
to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, 
and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: 
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, 
WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
